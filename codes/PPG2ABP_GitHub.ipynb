{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFyb0y7PUJOo"
   },
   "source": [
    "# BP Prediction and ABP Estimation End-to-End Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9vTr2NhcGAA"
   },
   "source": [
    "# Test GPU (Optional)\n",
    "Before Starting, kindly check the available GPU from the Google Server, GPU model and other related information. It might help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3014,
     "status": "ok",
     "timestamp": 1661045721247,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "VUNJNtNxcFTF",
    "outputId": "b7f886b9-c977-4f44-b030-9a8b4c7261c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA enabled GPU Available? False\n",
      "GPU Number: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs CUDA enabled GPU Available?\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Number:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent GPU Index:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU Capability:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_capability(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:482\u001b[0m, in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n",
    "print(\"GPU Number:\", torch.cuda.device_count())\n",
    "print(\"Current GPU Index:\", torch.cuda.current_device())\n",
    "print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n",
    "print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n",
    "print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQbDnsIZUSx0"
   },
   "source": [
    "# Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29489,
     "status": "ok",
     "timestamp": 1661045750728,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "wg4kMbwxsH9h",
    "outputId": "cbf3bfbf-24d7-4152-cdf3-a98f5a5b313d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/GDrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpdYZ2x3MbTd"
   },
   "source": [
    "Move to the Target Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1910,
     "status": "ok",
     "timestamp": 1661045752637,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "Efh543dRsQga",
    "outputId": "c7d73e9e-103f-48ea-e06a-3d0acaa40f1e"
   },
   "outputs": [],
   "source": [
    "%cd \"/content/GDrive/MyDrive/Colab_Notebooks/Research/PPG2ABP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZtMdxE7MeXX"
   },
   "source": [
    "List the Files and Folders Located in the Current Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1661045753625,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "eM92ZPcisMK1",
    "outputId": "6d0e6613-2573-452f-e446-b1b062bd61a7"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ano69se-e3QY"
   },
   "source": [
    "## Evaluation of Predicting ABP Waveforms\n",
    "\n",
    "Here, we present an interactive CLI to predict the ABP waveform from PPG signal from the test data. Ground truth, prediction from approximation network and refinement network are presented, and a comparison is also demonstrated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgW7r0C9TuZk"
   },
   "source": [
    "#Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eMhBhz1CrMb3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import scipy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZAo2XBvzFX49"
   },
   "outputs": [],
   "source": [
    "from ml_models import *\n",
    "from Evaluation_Metrics_ABP import *\n",
    "from UNet_1DCNN import UNet\n",
    "from BCDUNet_1DCNN import BCDUNet\n",
    "from SEDUNet_1DCNN import SEDUNet\n",
    "from Dense_Inception_UNet_1DCNN import Dense_Inception_UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "03JA1kRfzoit"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4tWmjn_kMZu"
   },
   "source": [
    "# Set Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YZtdPlqmQ7Tj"
   },
   "outputs": [],
   "source": [
    "num_channel = 4\n",
    "fold_num = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kir80l1FKPXB"
   },
   "source": [
    "# Import and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nujkRoUsuhhh"
   },
   "source": [
    "Note: Data pre-processing was mainly conducted in MATLAB. Here, data was loaded from the GDrive and prepared for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMHdM26iekBm"
   },
   "source": [
    "### Import Dataset for Train and Test on UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661045764772,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "KjIqJqKSrhrA",
    "outputId": "47085c6f-c618-4fd1-b71b-2b148d521968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47441, 1024)\n",
      "(53001, 1024)\n",
      "(40110, 1024)\n"
     ]
    }
   ],
   "source": [
    "fl_Train_1 = h5py.File(os.path.join('UCI_Dataset_Part_1_Preprocessed (1).h5'), 'r')\n",
    "fl_Train_2 = h5py.File(os.path.join('UCI_Dataset_Part_2_Preprocessed (1).h5'), 'r')\n",
    "fl_Train_3 = h5py.File(os.path.join('UCI_Dataset_Part_3_Preprocessed.h5'), 'r')\n",
    "print(fl_Train_1['PPG'].shape)\n",
    "print(fl_Train_2['PPG'].shape)\n",
    "print(fl_Train_3['PPG'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1309,
     "status": "ok",
     "timestamp": 1661045766901,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "gtfOyJzPrhzu",
    "outputId": "8d038384-1e1c-4167-e56c-c391a2b2cd47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50646, 1024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_Test = h5py.File(os.path.join('UCI_Dataset_Part_4_Preprocessed.h5'), 'r')  # load UCI Test Data\n",
    "fl_Test['PPG'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUciLEJeyvyN"
   },
   "source": [
    "### Prepare Train Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347306,
     "status": "ok",
     "timestamp": 1661046114202,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "myxmT5v08DL6",
    "outputId": "4859a20b-8134-4055-b533-e381260f1d99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Train Data Part 1: 100%|██████████████████████████████████████████████| 47441/47441 [04:16<00:00, 184.78it/s]\n",
      "Preparing Train Data Part 2: 100%|██████████████████████████████████████████████| 53001/53001 [04:36<00:00, 191.50it/s]\n",
      "Preparing Train Data Part 3: 100%|██████████████████████████████████████████████| 40110/40110 [03:53<00:00, 171.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140552, 1024, 4)\n",
      "(140552, 1024, 1)\n",
      "(140552, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "# intialize train data\n",
    "length = 1024\n",
    "X_Train_All = []\n",
    "Y_Train_BP_All = []\n",
    "Y_Train_ABP_All = []\n",
    "PPG_All = []\n",
    "APG_All = []\n",
    "VPG_All = []\n",
    "ECG_All = []\n",
    "\n",
    "if num_channel == 1:\n",
    "\n",
    "    X_Train = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_1['PPG'].shape[0]), desc='Preparing Train Data Part 1'):\n",
    "        X_Train.append(np.array(fl_Train_1['PPG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_1['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_1['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "    \n",
    "    X_Train = np.array(X_Train)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    X_Train_All = X_Train\n",
    "    Y_Train_BP_All = Y_Train_BP\n",
    "    Y_Train_ABP_All = Y_Train_ABP\n",
    "\n",
    "    X_Train = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_2['PPG'].shape[0]), desc='Preparing Train Data Part 2'):\n",
    "        X_Train.append(np.array(fl_Train_2['PPG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_2['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_2['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    X_Train = np.array(X_Train)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    X_Train_All = np.concatenate([X_Train_All, X_Train], axis=0)\n",
    "    Y_Train_BP_All = np.concatenate([Y_Train_BP_All, Y_Train_BP], axis=0)\n",
    "    Y_Train_ABP_All = np.concatenate([Y_Train_ABP_All, Y_Train_ABP], axis=0)\n",
    "    \n",
    "    X_Train = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_3['PPG'].shape[0]), desc='Preparing Train Data  Part 3'):\n",
    "        X_Train.append(np.array(fl_Train_3['PPG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_3['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_3['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    X_Train = np.array(X_Train)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    X_Train_All = np.concatenate([X_Train_All, X_Train], axis=0)\n",
    "    Y_Train_BP_All = np.concatenate([Y_Train_BP_All, Y_Train_BP], axis=0)\n",
    "    Y_Train_ABP_All = np.concatenate([Y_Train_ABP_All, Y_Train_ABP], axis=0)\n",
    "    \n",
    "    X_Train_All = np.array(X_Train_All)\n",
    "    Y_Train_BP_All = np.array(Y_Train_BP_All)\n",
    "    Y_Train_ABP_All = np.array(Y_Train_ABP_All)\n",
    "    print(X_Train_All.shape)\n",
    "    print(Y_Train_BP_All.shape)\n",
    "    print(Y_Train_ABP_All.shape)\n",
    "\n",
    "elif num_channel == 2:\n",
    "    PPG = []\n",
    "    ECG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_1['PPG'].shape[0]), desc='Preparing Train Data Part 1'):\n",
    "        PPG.append(np.array(fl_Train_1['PPG'][i][:length]).reshape(length, 1))\n",
    "        ECG.append(np.array(fl_Train_1['ECG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_1['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_1['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    ECG = np.array(ECG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = PPG\n",
    "    ECG_All = ECG\n",
    "    Y_Train_BP_All = Y_Train_BP\n",
    "    Y_Train_ABP_All = Y_Train_ABP\n",
    "\n",
    "    PPG = []\n",
    "    ECG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_2['PPG'].shape[0]), desc='Preparing Train Data Part 2'):\n",
    "        PPG.append(np.array(fl_Train_2['PPG'][i][:length]).reshape(length, 1))\n",
    "        ECG.append(np.array(fl_Train_2['ECG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_2['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_2['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    ECG = np.array(ECG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = np.concatenate([PPG_All, PPG], axis=0)\n",
    "    ECG_All = np.concatenate([ECG_All, ECG], axis=0)\n",
    "    Y_Train_BP_All = np.concatenate([Y_Train_BP_All, Y_Train_BP], axis=0)\n",
    "    Y_Train_ABP_All = np.concatenate([Y_Train_ABP_All, Y_Train_ABP], axis=0)\n",
    "\n",
    "    PPG = []\n",
    "    ECG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_3['PPG'].shape[0]), desc='Preparing Train Data Part 3'):\n",
    "        PPG.append(np.array(fl_Train_3['PPG'][i][:length]).reshape(length, 1))\n",
    "        ECG.append(np.array(fl_Train_3['ECG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_3['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_3['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    ECG = np.array(ECG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = np.concatenate([PPG_All, PPG], axis=0)\n",
    "    ECG_All = np.concatenate([ECG_All, ECG], axis=0)\n",
    "    Y_Train_BP_All = np.concatenate([Y_Train_BP_All, Y_Train_BP], axis=0)\n",
    "    Y_Train_ABP_All = np.concatenate([Y_Train_ABP_All, Y_Train_ABP], axis=0)\n",
    "\n",
    "    PPG_All = np.array(PPG_All)\n",
    "    ECG_All = np.array(ECG_All)\n",
    "    X_Train_All = np.squeeze(np.stack((PPG_All, ECG_All), axis=2))\n",
    "    Y_Train_BP_All = np.array(Y_Train_BP_All)\n",
    "    Y_Train_ABP_All = np.array(Y_Train_ABP_All)\n",
    "    print(X_Train_All.shape)\n",
    "    print(Y_Train_BP_All.shape)\n",
    "    print(Y_Train_ABP_All.shape)\n",
    "\n",
    "elif num_channel == 3:\n",
    "    PPG = []\n",
    "    VPG = []\n",
    "    APG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_1['PPG'].shape[0]), desc='Preparing Train Data Part 1'):\n",
    "        PPG.append(np.array(fl_Train_1['PPG'][i][:length]).reshape(length, 1))\n",
    "        VPG.append(np.array(fl_Train_1['VPG'][i][:length]).reshape(length, 1))\n",
    "        APG.append(np.array(fl_Train_1['APG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_1['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_1['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    VPG = np.array(VPG)\n",
    "    APG = np.array(APG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = PPG\n",
    "    VPG_All = VPG\n",
    "    APG_All = APG\n",
    "    Y_Train_BP_All = Y_Train_BP\n",
    "    Y_Train_ABP_All = Y_Train_ABP\n",
    "\n",
    "    PPG = []\n",
    "    VPG = []\n",
    "    APG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_2['PPG'].shape[0]), desc='Preparing Train Data Part 2'):\n",
    "        PPG.append(np.array(fl_Train_2['PPG'][i][:length]).reshape(length, 1))\n",
    "        VPG.append(np.array(fl_Train_2['VPG'][i][:length]).reshape(length, 1))\n",
    "        APG.append(np.array(fl_Train_2['APG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_2['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_2['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    VPG = np.array(VPG)\n",
    "    APG = np.array(APG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = np.concatenate([PPG_All, PPG], axis=0)\n",
    "    VPG_All = np.concatenate([VPG_All, VPG], axis=0)\n",
    "    APG_All = np.concatenate([APG_All, APG], axis=0)\n",
    "    Y_Train_BP_All = np.concatenate([Y_Train_BP_All, Y_Train_BP], axis=0)\n",
    "    Y_Train_ABP_All = np.concatenate([Y_Train_ABP_All, Y_Train_ABP], axis=0)\n",
    "\n",
    "    PPG = []\n",
    "    VPG = []\n",
    "    APG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_3['PPG'].shape[0]), desc='Preparing Train Data Part 3'):\n",
    "        PPG.append(np.array(fl_Train_3['PPG'][i][:length]).reshape(length, 1))\n",
    "        VPG.append(np.array(fl_Train_3['VPG'][i][:length]).reshape(length, 1))\n",
    "        APG.append(np.array(fl_Train_3['APG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_3['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_3['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    VPG = np.array(VPG)\n",
    "    APG = np.array(APG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = np.concatenate([PPG_All, PPG], axis=0)\n",
    "    VPG_All = np.concatenate([VPG_All, VPG], axis=0)\n",
    "    APG_All = np.concatenate([APG_All, APG], axis=0)\n",
    "    Y_Train_BP_All = np.concatenate([Y_Train_BP_All, Y_Train_BP], axis=0)\n",
    "    Y_Train_ABP_All = np.concatenate([Y_Train_ABP_All, Y_Train_ABP], axis=0)\n",
    "\n",
    "    PPG_All = np.array(PPG_All)\n",
    "    VPG_All = np.array(VPG_All)\n",
    "    APG_All = np.array(APG_All)\n",
    "    X_Train_All = np.squeeze(np.stack((PPG_All, VPG_All, APG_All), axis=2))\n",
    "    Y_Train_BP_All = np.array(Y_Train_BP_All)\n",
    "    Y_Train_ABP_All = np.array(Y_Train_ABP_All)\n",
    "    print(X_Train_All.shape)\n",
    "    print(Y_Train_BP_All.shape)\n",
    "    print(Y_Train_ABP_All.shape)\n",
    "\n",
    "elif num_channel == 4:\n",
    "    PPG = []\n",
    "    VPG = []\n",
    "    APG = []\n",
    "    ECG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_1['PPG'].shape[0]), desc='Preparing Train Data Part 1'):\n",
    "        PPG.append(np.array(fl_Train_1['PPG'][i][:length]).reshape(length, 1))\n",
    "        VPG.append(np.array(fl_Train_1['VPG'][i][:length]).reshape(length, 1))\n",
    "        APG.append(np.array(fl_Train_1['APG'][i][:length]).reshape(length, 1))\n",
    "        ECG.append(np.array(fl_Train_1['ECG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_1['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_1['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    VPG = np.array(VPG)\n",
    "    APG = np.array(APG)\n",
    "    ECG = np.array(ECG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = PPG\n",
    "    VPG_All = VPG\n",
    "    APG_All = APG\n",
    "    ECG_All = ECG\n",
    "    Y_Train_BP_All = Y_Train_BP\n",
    "    Y_Train_ABP_All = Y_Train_ABP\n",
    "\n",
    "    PPG = []\n",
    "    VPG = []\n",
    "    APG = []\n",
    "    ECG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_2['PPG'].shape[0]), desc='Preparing Train Data Part 2'):\n",
    "        PPG.append(np.array(fl_Train_2['PPG'][i][:length]).reshape(length, 1))\n",
    "        VPG.append(np.array(fl_Train_2['VPG'][i][:length]).reshape(length, 1))\n",
    "        APG.append(np.array(fl_Train_2['APG'][i][:length]).reshape(length, 1))\n",
    "        ECG.append(np.array(fl_Train_2['ECG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_2['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_2['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    VPG = np.array(VPG)\n",
    "    APG = np.array(APG)\n",
    "    ECG = np.array(ECG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = np.concatenate([PPG_All, PPG], axis=0)\n",
    "    VPG_All = np.concatenate([VPG_All, VPG], axis=0)\n",
    "    APG_All = np.concatenate([APG_All, APG], axis=0)\n",
    "    ECG_All = np.concatenate([ECG_All, ECG], axis=0)\n",
    "    Y_Train_BP_All = np.concatenate([Y_Train_BP_All, Y_Train_BP], axis=0)\n",
    "    Y_Train_ABP_All = np.concatenate([Y_Train_ABP_All, Y_Train_ABP], axis=0)\n",
    "\n",
    "    PPG = []\n",
    "    VPG = []\n",
    "    APG = []\n",
    "    ECG = []\n",
    "    Y_Train_BP = []\n",
    "    Y_Train_ABP = []\n",
    "    for i in tqdm(range(0, fl_Train_3['PPG'].shape[0]), desc='Preparing Train Data Part 3'):\n",
    "        PPG.append(np.array(fl_Train_3['PPG'][i][:length]).reshape(length, 1))\n",
    "        VPG.append(np.array(fl_Train_3['VPG'][i][:length]).reshape(length, 1))\n",
    "        APG.append(np.array(fl_Train_3['APG'][i][:length]).reshape(length, 1))\n",
    "        ECG.append(np.array(fl_Train_3['ECG'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_BP.append(np.array(fl_Train_3['ABP'][i][:length]).reshape(length, 1))\n",
    "        Y_Train_ABP.append(np.array(fl_Train_3['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    VPG = np.array(VPG)\n",
    "    APG = np.array(APG)\n",
    "    ECG = np.array(ECG)\n",
    "    Y_Train_BP = np.array(Y_Train_BP)\n",
    "    Y_Train_ABP = np.array(Y_Train_ABP)\n",
    "    PPG_All = np.concatenate([PPG_All, PPG], axis=0)\n",
    "    VPG_All = np.concatenate([VPG_All, VPG], axis=0)\n",
    "    APG_All = np.concatenate([APG_All, APG], axis=0)\n",
    "    ECG_All = np.concatenate([ECG_All, ECG], axis=0)\n",
    "    Y_Train_BP_All = np.concatenate([Y_Train_BP_All, Y_Train_BP], axis=0)\n",
    "    Y_Train_ABP_All = np.concatenate([Y_Train_ABP_All, Y_Train_ABP], axis=0)\n",
    "\n",
    "    PPG_All = np.array(PPG_All)\n",
    "    VPG_All = np.array(VPG_All)\n",
    "    APG_All = np.array(APG_All)\n",
    "    ECG_All = np.array(ECG_All)\n",
    "    X_Train_All = np.squeeze(np.stack((PPG_All, VPG_All, APG_All, ECG_All), axis=2))\n",
    "    Y_Train_BP_All = np.array(Y_Train_BP_All)\n",
    "    Y_Train_ABP_All = np.array(Y_Train_ABP_All)\n",
    "    #\n",
    "    print(X_Train_All.shape)\n",
    "    print(Y_Train_BP_All.shape)\n",
    "    print(Y_Train_ABP_All.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vyX1xJAzDLF"
   },
   "source": [
    "### Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127233,
     "status": "ok",
     "timestamp": 1661046241424,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "K73qn7Yly-q8",
    "outputId": "3962c69c-70d6-49a5-f0b8-4b8eb39fc66c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Test Data: 100%|██████████████████████████████████████████████████████| 50646/50646 [04:19<00:00, 194.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50646, 1024, 4, 1)\n",
      "(50646, 1024, 1)\n",
      "(50646, 1024, 1)\n"
     ]
    }
   ],
   "source": [
    "# intialize test data\n",
    "\n",
    "length = 1024\n",
    "X_Test = []\n",
    "Y_Test_BP = []\n",
    "Y_Test_ABP = []\n",
    "\n",
    "if num_channel == 1:\n",
    "\n",
    "    for i in tqdm(range(0, fl_Test['PPG'].shape[0]), desc='Preparing Test Data'):\n",
    "        X_Test.append(np.array(fl_Test['PPG'][i][:length]).reshape(length, 1))    # ppg signal\n",
    "        Y_Test_BP.append(np.array(fl_Test['ABP'][i][:length]).reshape(length, 1)) # abp signal\n",
    "        Y_Test_ABP.append(np.array(fl_Test['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "\n",
    "    X_Test = np.array(X_Test)\n",
    "    Y_Test_BP = np.array(Y_Test_BP)\n",
    "    Y_Test_ABP = np.array(Y_Test_ABP)\n",
    "    Y_Test_ABP.shape\n",
    "\n",
    "elif num_channel == 2:\n",
    "    PPG = []\n",
    "    ECG = []\n",
    "\n",
    "    for i in tqdm(range(0, fl_Test['PPG'].shape[0]), desc='Preparing Test Data'):\n",
    "        PPG.append(np.array(fl_Test['PPG'][i][:length]).reshape(length, 1))       # ppg signal\n",
    "        ECG.append(np.array(fl_Test['ECG'][i][:length]).reshape(length, 1))       # ecg signal\n",
    "        Y_Test_BP.append(np.array(fl_Test['ABP'][i][:length]).reshape(length, 1)) # abp signal\n",
    "        Y_Test_ABP.append(np.array(fl_Test['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    ECG = np.array(ECG)\n",
    "    X_Test = np.stack((PPG, ECG), axis=2)\n",
    "    Y_Test_BP = np.array(Y_Test_BP)\n",
    "    Y_Test_ABP = np.array(Y_Test_ABP)\n",
    "    Y_Test_ABP.shape\n",
    "\n",
    "elif num_channel == 3:\n",
    "    PPG = []\n",
    "    VPG = []\n",
    "    APG = []\n",
    "\n",
    "    for i in tqdm(range(0, fl_Test['PPG'].shape[0]), desc='Preparing Test Data'):\n",
    "        PPG.append(np.array(fl_Test['PPG'][i][:length]).reshape(length, 1))       # ppg signal\n",
    "        VPG.append(np.array(fl_Test['VPG'][i][:length]).reshape(length, 1))       # vpg signal\n",
    "        APG.append(np.array(fl_Test['APG'][i][:length]).reshape(length, 1))       # apg signal\n",
    "        Y_Test_BP.append(np.array(fl_Test['ABP'][i][:length]).reshape(length, 1)) # abp signal\n",
    "        Y_Test_ABP.append(np.array(fl_Test['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    VPG = np.array(VPG)\n",
    "    APG = np.array(APG)\n",
    "    X_Test = np.stack((PPG, VPG, APG), axis=2)\n",
    "    Y_Test_BP = np.array(Y_Test_BP)\n",
    "    Y_Test_ABP = np.array(Y_Test_ABP)\n",
    "    Y_Test_ABP.shape\n",
    "\n",
    "elif num_channel == 4:\n",
    "    PPG = []\n",
    "    VPG = []\n",
    "    APG = []\n",
    "    ECG = []\n",
    "\n",
    "    for i in tqdm(range(0, fl_Test['PPG'].shape[0]), desc='Preparing Test Data'):\n",
    "        PPG.append(np.array(fl_Test['PPG'][i][:length]).reshape(length, 1))       # ppg signal\n",
    "        VPG.append(np.array(fl_Test['VPG'][i][:length]).reshape(length, 1))       # vpg signal\n",
    "        APG.append(np.array(fl_Test['APG'][i][:length]).reshape(length, 1))       # apg signal\n",
    "        ECG.append(np.array(fl_Test['ECG'][i][:length]).reshape(length, 1))       # apg signal\n",
    "        Y_Test_BP.append(np.array(fl_Test['ABP'][i][:length]).reshape(length, 1)) # abp signal\n",
    "        Y_Test_ABP.append(np.array(fl_Test['ABP_RNorm'][i][:length]).reshape(length, 1))\n",
    "\n",
    "\n",
    "    PPG = np.array(PPG)\n",
    "    VPG = np.array(VPG)\n",
    "    APG = np.array(APG)\n",
    "    ECG = np.array(ECG)\n",
    "    X_Test = np.stack((PPG, VPG, APG, ECG), axis=2)\n",
    "    Y_Test_BP = np.array(Y_Test_BP)\n",
    "    Y_Test_ABP = np.array(Y_Test_ABP)\n",
    "    #\n",
    "    print(X_Test.shape)\n",
    "    print(Y_Test_BP.shape)\n",
    "    print(Y_Test_ABP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZH9ESc6ZKKiG"
   },
   "source": [
    "### Extract SBP and DBP Label Data from the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82360,
     "status": "ok",
     "timestamp": 1661046323769,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "HufmOkyqKKiG",
    "outputId": "28e13383-9d57-41cf-bfea-712bf29a2247"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Ground_Truth SBP from Train Data Part 1: 100%|██████████████████████| 47441/47441 [00:19<00:00, 2445.77it/s]\n",
      "Extracting Ground_Truth SBP from Train Data Part 2: 100%|██████████████████████| 53001/53001 [00:21<00:00, 2511.25it/s]\n",
      "Extracting Ground_Truth SBP from Train Data Part 3: 100%|██████████████████████| 40110/40110 [00:16<00:00, 2390.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140552,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Ground_Truth DBP from Train Data Part 1: 100%|██████████████████████| 47441/47441 [00:21<00:00, 2239.73it/s]\n",
      "Extracting Ground_Truth DBP from Train Data Part 2: 100%|██████████████████████| 53001/53001 [00:21<00:00, 2454.30it/s]\n",
      "Extracting Ground_Truth DBP from Train Data Part 3: 100%|██████████████████████| 40110/40110 [00:18<00:00, 2201.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140552,)\n"
     ]
    }
   ],
   "source": [
    "SBP_Train_1 = []\n",
    "SBP_Train_2 = []\n",
    "SBP_Train_3 = []\n",
    "DBP_Train_1 = []\n",
    "DBP_Train_2 = []\n",
    "DBP_Train_3 = []\n",
    "\n",
    "# SBP\n",
    "for i in tqdm(range(0, fl_Train_1['SBP'].shape[0]), desc='Extracting Ground_Truth SBP from Train Data Part 1'):\n",
    "    SBP_Train_1.append(np.array(fl_Train_1['SBP'][i][0]).reshape(1, 1)) \n",
    "SBP_Train_1 = np.squeeze(np.array(SBP_Train_1), axis=2)\n",
    "\n",
    "for i in tqdm(range(0, fl_Train_2['SBP'].shape[0]), desc='Extracting Ground_Truth SBP from Train Data Part 2'):\n",
    "    SBP_Train_2.append(np.array(fl_Train_2['SBP'][i][0]).reshape(1, 1)) \n",
    "SBP_Train_2 = np.squeeze(np.array(SBP_Train_2), axis=2)\n",
    "\n",
    "for i in tqdm(range(0, fl_Train_3['SBP'].shape[0]), desc='Extracting Ground_Truth SBP from Train Data Part 3'):\n",
    "    SBP_Train_3.append(np.array(fl_Train_3['SBP'][i][0]).reshape(1, 1)) \n",
    "SBP_Train_3 = np.squeeze(np.array(SBP_Train_3), axis=2)\n",
    "\n",
    "SBP_Train = np.squeeze(np.concatenate((SBP_Train_1, SBP_Train_2, SBP_Train_3), axis=0))\n",
    "print(SBP_Train.shape)\n",
    "\n",
    "# DBP\n",
    "for i in tqdm(range(0, fl_Train_1['DBP'].shape[0]), desc='Extracting Ground_Truth DBP from Train Data Part 1'):\n",
    "    DBP_Train_1.append(np.array(fl_Train_1['DBP'][i][0]).reshape(1, 1)) \n",
    "DBP_Train_1 = np.squeeze(np.array(DBP_Train_1), axis=2)\n",
    "\n",
    "for i in tqdm(range(0, fl_Train_2['DBP'].shape[0]), desc='Extracting Ground_Truth DBP from Train Data Part 2'):\n",
    "    DBP_Train_2.append(np.array(fl_Train_2['DBP'][i][0]).reshape(1, 1)) \n",
    "DBP_Train_2 = np.squeeze(np.array(DBP_Train_2), axis=2)\n",
    "\n",
    "for i in tqdm(range(0, fl_Train_3['DBP'].shape[0]), desc='Extracting Ground_Truth DBP from Train Data Part 3'):\n",
    "    DBP_Train_3.append(np.array(fl_Train_3['DBP'][i][0]).reshape(1, 1)) \n",
    "DBP_Train_3 = np.squeeze(np.array(DBP_Train_3), axis=2)\n",
    "\n",
    "DBP_Train = np.squeeze(np.concatenate((DBP_Train_1, DBP_Train_2, DBP_Train_3), axis=0))\n",
    "#\n",
    "print(DBP_Train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU5by9jmKKiG"
   },
   "source": [
    "### Extract Ground Truth SBP and DBP from the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33114,
     "status": "ok",
     "timestamp": 1661046356877,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "JpJdqLQ3KKiG",
    "outputId": "b55dba32-0487-4c97-923a-972db0207603"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Ground_Truth SBP from Testing Set: 100%|████████████████████████████| 50646/50646 [00:39<00:00, 1295.09it/s]\n",
      "Extracting Ground_Truth DBP from Testing Set: 100%|████████████████████████████| 50646/50646 [00:18<00:00, 2778.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50646, 1)\n",
      "(50646, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SBP_Test = []\n",
    "DBP_Test = []\n",
    "\n",
    "# SBP\n",
    "for i in tqdm(range(0, fl_Test['SBP'].shape[0]), desc='Extracting Ground_Truth SBP from Testing Set'):\n",
    "    SBP_Test.append(np.array(fl_Test['SBP'][i][0]).reshape(1, 1)) \n",
    "\n",
    "SBP_Test = np.squeeze(np.array(SBP_Test),axis=2)\n",
    "\n",
    "# DBP\n",
    "for i in tqdm(range(0, fl_Test['DBP'].shape[0]), desc='Extracting Ground_Truth DBP from Testing Set'):\n",
    "    DBP_Test.append(np.array(fl_Test['DBP'][i][0]).reshape(1, 1)) \n",
    "\n",
    "DBP_Test = np.squeeze(np.array(DBP_Test),axis=2)\n",
    "#\n",
    "print(SBP_Test.shape)\n",
    "print(DBP_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rggieN8mqJu"
   },
   "source": [
    "### Prepare ABP Ground Truth for PPG2ABP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19955,
     "status": "ok",
     "timestamp": 1661046376819,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "D_gYyZhQmqS1",
    "outputId": "7c8cdfa5-0086-4050-8ffc-b3f069c2cc9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Ground_Truth: 100%|█████████████████████████████████████████████████| 50646/50646 [00:26<00:00, 1915.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50646, 1024, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABP_GRND = []\n",
    "\n",
    "for i in tqdm(range(0, fl_Test['ABP_GRND'].shape[0]), desc='Extracting Ground_Truth'):\n",
    "    ABP_GRND.append(np.array(fl_Test['ABP_GRND'][i][:length]).reshape(length, 1)) \n",
    "\n",
    "ABP_GRND = np.array(ABP_GRND)\n",
    "ABP_GRND.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vecQm7YeKKiH"
   },
   "source": [
    "### Train-Val Split [Test Data is Completely Independent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tFLWB7dlKKiH"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 878. MiB for an array with shape (112441, 1024, 1) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_Train1, X_Val1, Y_Train1, Y_Val1, Y_Train2, Y_Val2, SBP_Train1, SBP_Val1, DBP_Train1, DBP_Val1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_Train_All\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_Train_BP_All\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_Train_ABP_All\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSBP_Train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDBP_Train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2585\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m-> 2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2587\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2586\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2587\u001b[0m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:356\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:185\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    184\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 878. MiB for an array with shape (112441, 1024, 1) and data type float64"
     ]
    }
   ],
   "source": [
    "X_Train1, X_Val1, Y_Train1, Y_Val1, Y_Train2, Y_Val2, SBP_Train1, SBP_Val1, DBP_Train1, DBP_Val1 = train_test_split(X_Train_All, Y_Train_BP_All, Y_Train_ABP_All, SBP_Train, DBP_Train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZK3ZXFCAE1Z"
   },
   "outputs": [],
   "source": [
    "X_Test1 = X_Test\n",
    "Y_Test1 = Y_Test_BP\n",
    "Y_Test2 = Y_Test_ABP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjAiGbvq_3rj"
   },
   "source": [
    "### Save Splitted UCI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7Ct_aOM-twX"
   },
   "outputs": [],
   "source": [
    "if num_channel == 1:\n",
    "  hf = h5py.File('UCI_Splitted_Data_1CH.h5', 'w')\n",
    "\n",
    "elif num_channel == 2:\n",
    "  hf = h5py.File('UCI_Splitted_Data_2CH.h5', 'w')\n",
    "\n",
    "elif num_channel == 3:\n",
    "  hf = h5py.File('UCI_Splitted_Data_3CH.h5', 'w')\n",
    "\n",
    "elif num_channel == 4:\n",
    "  hf = h5py.File('UCI_Splitted_Data_4CH.h5', 'w')\n",
    "#\n",
    "hf.create_dataset('X_Train1', data=X_Train1)\n",
    "hf.create_dataset('X_Val1', data=X_Val1)\n",
    "hf.create_dataset('Y_Train1', data=Y_Train1)\n",
    "hf.create_dataset('Y_Val1', data=Y_Val1)\n",
    "hf.create_dataset('Y_Train2', data=Y_Train2)\n",
    "hf.create_dataset('Y_Val2', data=Y_Val2)\n",
    "hf.create_dataset('X_Test1', data=X_Test1)\n",
    "hf.create_dataset('Y_Test1', data=Y_Test1)\n",
    "hf.create_dataset('Y_Test2', data=Y_Test2)\n",
    "hf.create_dataset('SBP_Train1', data=SBP_Train1)\n",
    "hf.create_dataset('SBP_Val1', data=SBP_Val1)\n",
    "hf.create_dataset('DBP_Train1', data=DBP_Train1)\n",
    "hf.create_dataset('DBP_Val1', data=DBP_Val1)\n",
    "hf.create_dataset('SBP_Test', data=SBP_Test)\n",
    "hf.create_dataset('DBP_Test', data=DBP_Test)\n",
    "hf.create_dataset('ABP_GRND', data=ABP_GRND)\n",
    "#\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxheQiz4pIRK"
   },
   "source": [
    "### Import Splitted UCI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuyY3CewpIRN"
   },
   "outputs": [],
   "source": [
    "if num_channel == 1:\n",
    "  hf = h5py.File('UCI_Splitted_Data_1CH.h5', 'r')\n",
    "  hf.keys()\n",
    "\n",
    "elif num_channel == 2:\n",
    "  hf = h5py.File('UCI_Splitted_Data_2CH.h5', 'r')\n",
    "  hf.keys()\n",
    "\n",
    "elif num_channel == 3:\n",
    "  hf = h5py.File('UCI_Splitted_Data_3CH.h5', 'r')\n",
    "  hf.keys()\n",
    "\n",
    "elif num_channel == 4:\n",
    "  hf = h5py.File('UCI_Splitted_Data_4CH.h5', 'r')\n",
    "  hf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zu42iybxdJ0"
   },
   "outputs": [],
   "source": [
    "X_Train1 = np.array(hf.get('X_Train1'))\n",
    "X_Val1 = np.array(hf.get('X_Val1'))\n",
    "Y_Train1 = np.array(hf.get('Y_Train1'))\n",
    "Y_Val1 = np.array(hf.get('Y_Val1'))\n",
    "Y_Train2 = np.array(hf.get('Y_Train2'))\n",
    "Y_Val2 = np.array(hf.get('Y_Val2'))\n",
    "SBP_Train1 = np.array(hf.get('SBP_Train1'))\n",
    "SBP_Val1 = np.array(hf.get('SBP_Val1'))\n",
    "DBP_Train1 = np.array(hf.get('DBP_Train1'))\n",
    "DBP_Val1 = np.array(hf.get('DBP_Val1'))\n",
    "SBP_Test = np.array(hf.get('SBP_Test'))\n",
    "DBP_Test = np.array(hf.get('DBP_Test'))\n",
    "ABP_GRND = np.array(hf.get('ABP_GRND'))\n",
    "X_Test1 = np.array(hf.get('X_Test1'))\n",
    "Y_Test1 = np.array(hf.get('Y_Test1'))\n",
    "Y_Test2 = np.array(hf.get('Y_Test2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qTwozk_BS94"
   },
   "source": [
    "### Garbage Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661046384142,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "ch2jmn3jKKiH",
    "outputId": "ff8d47c4-f768-4d52-d8ba-a3e7851c7054"
   },
   "outputs": [],
   "source": [
    "import gc #Garbage Collector\n",
    "PPG = None\n",
    "VPG = None\n",
    "APG = None\n",
    "ECG = None\n",
    "PPG_All = None\n",
    "VPG_All = None\n",
    "APG_All = None\n",
    "ECG_All = None\n",
    "X_Train_All = None\n",
    "Y_Train_BP_All = None\n",
    "Y_Train_ABP_All = None\n",
    "fl_Train = None\n",
    "fl_Test = None\n",
    "X_Test = None\n",
    "SBP_Train_1 = None\n",
    "SBP_Train_2 = None\n",
    "SBP_Train_3 = None\n",
    "DBP_Train_1 = None\n",
    "DBP_Train_2 = None\n",
    "DBP_Train_3 = None\n",
    "SBP_Train = None\n",
    "DBP_Train = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHq0FrX9iAsq"
   },
   "source": [
    "# ABP Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yozhkF-OJWu2"
   },
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TVRg-mXF9gS"
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "signal_length = 1024  # Length of each Segment\n",
    "model_depth = 5  # Number of Level in the CNN Model\n",
    "model_width = 64  # Width of the Initial Layer, subsequent layers start from here\n",
    "kernel_size = 3  # Size of the Kernels/Filter\n",
    "num_channel = 4  # Number of Channels in the Model\n",
    "D_S = 0  # Turn on Deep Supervision\n",
    "A_E = 0  # Turn on AutoEncoder Mode for Feature Extraction\n",
    "A_G = 0  # Turn on for Guided Attention\n",
    "LSTM = 0\n",
    "num_dense_loop = 1\n",
    "problem_type = 'Regression'\n",
    "output_nums = 1  # Number of Class for Classification Problems, always '1' for Regression Problems\n",
    "feature_number = 1024  # Number of Features to be Extracted, only required if the AutoEncoder Mode is turned on\n",
    "model_name = 'NABNet'  # Dense_Inception_UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGoiZp6_JpIZ"
   },
   "source": [
    "## Prepare Data for Deep Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jms3ad41Joam"
   },
   "outputs": [],
   "source": [
    "def prepareTrainDict(y, model_depth, signal_length, model_name):\n",
    "  def approximate(inp, w_len, signal_length):\n",
    "    op = np.zeros((len(inp),signal_length//w_len))\n",
    "    for i in range(0,signal_length,w_len):\n",
    "      try:\n",
    "        op[:,i//w_len] = np.mean(inp[:,i:i+w_len],axis=1)\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        print(i)\n",
    "  \t\n",
    "    return op\n",
    "\n",
    "  out = {}\n",
    "  Y_Train_dict = {}\n",
    "  out['out'] = np.array(y)\n",
    "  Y_Train_dict['out'] = out['out']\n",
    "  for i in range(1, (model_depth+1)):\n",
    "    name = f'level{i}'\n",
    "    if (model_name == 'UNet'):\n",
    "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**i, signal_length),axis = 2)\n",
    "    elif (model_name == 'UNetPP'):\n",
    "      out[name] = np.expand_dims(approximate(np.squeeze(y), 2**0, signal_length),axis = 2)\n",
    "    Y_Train_dict[f'level{i}'] = out[f'level{i}']\n",
    "  \n",
    "  return out, Y_Train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYenayCwSFF8"
   },
   "outputs": [],
   "source": [
    "model_name_DS = 'UNetPP' # UNet or UNetPP (Two Types)\n",
    "X_Train3 = X_Train1\n",
    "X_Val3 = X_Val1\n",
    "[Y_Train3, Y_Train_dict] = prepareTrainDict(Y_Train2, model_depth, signal_length, model_name_DS)\n",
    "[Y_Val3, Y_Val_dict] = prepareTrainDict(Y_Val2, model_depth, signal_length, model_name_DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1660991657584,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "43oF2ywSSFcq",
    "outputId": "bb51f9a3-d722-451b-fda9-5688faf593b9"
   },
   "outputs": [],
   "source": [
    "loss_weights = np.zeros(model_depth)\n",
    "\n",
    "for i in range(0, model_depth):\n",
    "   loss_weights[i] = 1-(i*0.1)\n",
    "   \n",
    "loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660991657584,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "U1Nx4jdZWYAF",
    "outputId": "bce21381-4ef5-4f96-8a14-021d7df153d4"
   },
   "outputs": [],
   "source": [
    "import gc #Garbage Collector\n",
    "X_Test1 = None\n",
    "X_Train1 = None\n",
    "X_Val1 = None\n",
    "Y_Train2 = None\n",
    "Y_Test2 = None\n",
    "Y_Val2 = None\n",
    "Y_Train3 = None\n",
    "Y_Val3 = None\n",
    "PAVE2ABP_Network = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWQQ-U2bJ2Js"
   },
   "source": [
    "## Build Segmentation Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSLqe-8gJ2R8"
   },
   "outputs": [],
   "source": [
    "# Build model for PPG2ABP Segmentation - Deep UNet Architecture\n",
    "PAVE2ABP_Network = UNet(signal_length, model_depth, num_channel, model_width, kernel_size, problem_type=problem_type, output_nums=output_nums,\n",
    "                    ds=D_S, ae=A_E, ag=A_G, is_transconv=True).UNetPP()\n",
    "if D_S == 0:\n",
    "    PAVE2ABP_Network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss=tf.keras.losses.MeanSquaredError(), metrics=tf.keras.metrics.MeanSquaredError())\n",
    "elif D_S == 1:\n",
    "    PAVE2ABP_Network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss=tf.keras.losses.MeanSquaredError(), metrics=tf.keras.metrics.MeanSquaredError(), loss_weights= loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgJMiRUPy7aC"
   },
   "source": [
    "Load Previously Trained Weights to a Blank Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk0EwiXwK0NR"
   },
   "outputs": [],
   "source": [
    "trained_model_path = 'Trained_Models/PAVE2ABP_UCI_'+model_name+'_'+str(model_depth)+'_'+str(model_width)+'_'+str(kernel_size)+'_'+str(num_channel)+'_Fold_'+str(fold_num)+'.h5'\n",
    "PAVE2ABP_Network.load_weights(trained_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqm9NsbBEH7X"
   },
   "source": [
    "Or, Load Previously Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRflaj9sEIC9"
   },
   "outputs": [],
   "source": [
    "trained_model_path = 'Trained_Models/PAVE2ABP_UCI_'+model_name+'_'+str(model_depth)+'_'+str(model_width)+'_'+str(kernel_size)+'_'+str(num_channel)+'_Fold_'+str(fold_num)+'.h5'\n",
    "PAVE2ABP_Network = tf.keras.models.load_model(trained_model_path)\n",
    "if D_S == 0:\n",
    "  PAVE2ABP_Network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss=tf.keras.losses.MeanSquaredError(), metrics=tf.keras.metrics.MeanSquaredError())\n",
    "elif D_S == 1:\n",
    "  PAVE2ABP_Network.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss=tf.keras.losses.MeanSquaredError(), metrics=tf.keras.metrics.MeanSquaredError(), loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o6un6LPy-hp"
   },
   "source": [
    "Compile and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 58375080,
     "status": "error",
     "timestamp": 1661104914747,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "rSIIZU5vRaQs",
    "outputId": "77c1752b-6d04-4437-9959-f1b8cf561777"
   },
   "outputs": [],
   "source": [
    "if D_S == 0:\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, mode='min'), tf.keras.callbacks.ModelCheckpoint('Trained_Models/PAVE2ABP_UCI_'+model_name+'_'\n",
    "    +str(model_depth)+'_'+str(model_width)+'_'+str(kernel_size)+'_'+str(num_channel)+'_Fold_'+str(fold_num)+'.h5', verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
    "    \n",
    "    history = PAVE2ABP_Network.fit(X_Train1, Y_Train2, epochs=200, batch_size=32, verbose=1, validation_data= (X_Val1, Y_Val2), shuffle= True, callbacks= callbacks)\n",
    "elif D_S == 1:\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_out_loss', patience=15, mode='min'), tf.keras.callbacks.ModelCheckpoint('Trained_Models/PAVE2ABP_UCI_'+model_name+'_'\n",
    "    +str(model_depth)+'_'+str(model_width)+'_'+str(kernel_size)+'_'+str(num_channel)+'_Fold_'+str(fold_num)+'.h5', verbose=1, monitor='val_out_loss', save_best_only=True, mode='min')]\n",
    "    history = PAVE2ABP_Network.fit(X_Train3, Y_Train_dict, epochs=200, batch_size=8, verbose=1, validation_data= (X_Val3, Y_Val_dict), shuffle= True, callbacks= callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7NXUad0zRqO"
   },
   "source": [
    "Plot History Plots (e.g., Loss, Accuracy, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Deh8Zhr1Sb6c"
   },
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "  # list all dictionaries in history\n",
    "  print(history.history.keys())\n",
    "  # summarize history for error\n",
    "  plt.figure(figsize=(12,10))\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.plot(history.history['out_mean_squared_error'])\n",
    "  plt.plot(history.history['val_out_mean_squared_error'])\n",
    "  plt.title('Model Error Performance')\n",
    "  plt.ylabel('Error')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "  # summarize history for loss\n",
    "  plt.figure(figsize=(12,10))\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.plot(history.history['out_loss'])\n",
    "  plt.plot(history.history['val_out_loss'])\n",
    "  plt.title('Model Loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "#\n",
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyIdzHu1zWgm"
   },
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 193429,
     "status": "ok",
     "timestamp": 1661105113072,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "KJF-CYFzxnZc",
    "outputId": "9482b660-cd33-491b-b67d-1ad118fa47b1"
   },
   "outputs": [],
   "source": [
    "if D_S == 0:\n",
    "    ABP_App = PAVE2ABP_Network.predict(X_Test1, verbose=1)\n",
    "    print(ABP_App.shape)\n",
    "elif D_S == 1:\n",
    "    X_Test3 = X_Test1\n",
    "    ABP_App_MultiLevels = PAVE2ABP_Network.predict(X_Test3, verbose=1)\n",
    "    ABP_App = ABP_App_MultiLevels[0]\n",
    "    print(ABP_App.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNNUcrrSJX7K"
   },
   "source": [
    "Construction Error before Denormalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1661105113882,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "Xzcd5U4-I8dJ",
    "outputId": "3a16d7a2-a4ea-45c9-ada4-6965d09bfa4e"
   },
   "outputs": [],
   "source": [
    "[ABP_GRND_NEW, App_Predict_NEW] = Construction_Error_ABP(Y_Test2, ABP_App)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5wSF2COzYsK"
   },
   "source": [
    "Import BP Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MEUUOyyxob5"
   },
   "outputs": [],
   "source": [
    "infile = open('Trained_Models/Preds_SBP_Channel_' + str(num_channel) + '_Fold_' + str(fold_num) + '.p','rb')\n",
    "Preds_SBP_PAVE2BP = pickle.load(infile)\n",
    "infile.close()\n",
    "#\n",
    "infile = open('Trained_Models/Preds_DBP_Channel_' + str(num_channel) + '_Fold_' + str(fold_num) + '.p','rb')\n",
    "Preds_DBP_PAVE2BP = pickle.load(infile)\n",
    "infile.close()\n",
    "#\n",
    "SBP = Preds_SBP_PAVE2BP.ravel()\n",
    "DBP = Preds_DBP_PAVE2BP.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95251,
     "status": "ok",
     "timestamp": 1661105583949,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "mFPM4RBNqLyQ",
    "outputId": "71a62caa-b590-4b4e-f943-01a761687d5c"
   },
   "outputs": [],
   "source": [
    "ABP_App_Pred = []\n",
    "\n",
    "for i in tqdm(range(0, ABP_App.shape[0]), desc='Denormalizing ABP'):\n",
    "    ABP_App[i] = (ABP_App[i] - min(ABP_App[i]))/(max(ABP_App[i])-min(ABP_App[i])) # Range Normalize [0 1]\n",
    "    ABP_App_Pred.append((ABP_App[i]*(SBP[i] - DBP[i])) + DBP[i])\n",
    "\n",
    "ABP_App_Pred = np.array(ABP_App_Pred)\n",
    "print(ABP_App_Pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAkQe4Yp9QVP"
   },
   "source": [
    "Visualize Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1661105613344,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "tq3lBCm0crMc",
    "outputId": "e3b0f632-e221-4ce3-81ff-602bf5ac4fbf"
   },
   "outputs": [],
   "source": [
    "i = random.randint(0,len(ABP_GRND))\n",
    "MAE = np.mean(np.abs(ABP_App_Pred[i].ravel()-ABP_GRND[i].ravel()))\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(X_Test1[i,:,0].ravel(), label='PPG');\n",
    "plt.title(f\"PPG -- Sample Number {i}\")\n",
    "plt.legend();\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(Y_Test2[i].ravel(), label='ABP_GT_Norm');\n",
    "plt.plot(ABP_App[i].ravel(), label='Pred_Norm');\n",
    "plt.title(f\"ABP Normalized -- Sample Number {i}\")\n",
    "plt.legend();\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(ABP_GRND[i],label='ABP_GT');\n",
    "plt.plot(ABP_App_Pred[i].ravel(), label='Pred');\n",
    "plt.title(f\"ABP -- Sample Number {i} -- MAE = {MAE}\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bS8MG_sLnXRp"
   },
   "source": [
    "Save Predicted ABP Waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REjba3wPnXYT"
   },
   "outputs": [],
   "source": [
    "hf = h5py.File('Results/ABP_Estimated_Fold_1.h5', 'w')\n",
    "hf.create_dataset('ABP_RNorm', data=ABP_App)\n",
    "hf.create_dataset('ABP', data=ABP_App_Pred)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4fdSh4ZvH93"
   },
   "source": [
    "## Evaluate ABP Estimation Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auxnk2ZPzmSq"
   },
   "source": [
    "Construction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1661105645373,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "uP3RqZRivMsY",
    "outputId": "271b5dde-5301-472c-bbce-0fa8df5fd94c"
   },
   "outputs": [],
   "source": [
    "[ABP_GRND_NEW, App_Predict_NEW] = Construction_Error_ABP(ABP_GRND, ABP_App_Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs9ktFRSzqD5"
   },
   "source": [
    "BHS Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "executionInfo": {
     "elapsed": 29895,
     "status": "ok",
     "timestamp": 1661105675862,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "27bJBk5UvMu0",
    "outputId": "938d69a2-49e1-417f-e396-224f66d1f66d"
   },
   "outputs": [],
   "source": [
    "BHS_Metric_ABP(ABP_GRND_NEW, App_Predict_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7BahyPgzrUH"
   },
   "source": [
    "AAMI Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "executionInfo": {
     "elapsed": 16078,
     "status": "ok",
     "timestamp": 1661105691937,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "HOA6Cl4qyiYT",
    "outputId": "743bf71f-179f-463f-8ac4-3d0a4b21c3c7"
   },
   "outputs": [],
   "source": [
    "calcErrorAAMI_ABP(ABP_GRND_NEW, App_Predict_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgOF6zMVzt0L"
   },
   "source": [
    "Evaluate Blood Pressure Level Classification from Estimated ABP Waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 967
    },
    "executionInfo": {
     "elapsed": 15455,
     "status": "ok",
     "timestamp": 1661105707379,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "OwqowEwTyj_U",
    "outputId": "199a7f32-1a01-413d-cb19-b59cee1213b7"
   },
   "outputs": [],
   "source": [
    "evaluate_BP_Classification_ABP(ABP_GRND_NEW, App_Predict_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf5O4YLfz6Bb"
   },
   "source": [
    "Regression Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "executionInfo": {
     "elapsed": 23573,
     "status": "ok",
     "timestamp": 1661105730949,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "A09X4aimylv7",
    "outputId": "a9aecf56-1827-44bf-9c3d-26ba91f2b7cf"
   },
   "outputs": [],
   "source": [
    "regression_plot_ABP(ABP_GRND_NEW, App_Predict_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-k9amXy0D_e"
   },
   "source": [
    "Bland-Altman Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "executionInfo": {
     "elapsed": 12910,
     "status": "ok",
     "timestamp": 1661105743852,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "MYmvB8I5ynYu",
    "outputId": "ce34c3d6-3951-48e5-d416-e91ff1779fcd"
   },
   "outputs": [],
   "source": [
    "bland_altman_plot_ABP(ABP_GRND_NEW, App_Predict_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WesN7p7AeYK4"
   },
   "source": [
    "# Infinite Loop to Keep the Tab Alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HqQ3O6XeZ1f"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OjAiGbvq_3rj",
    "RxheQiz4pIRK",
    "_qTwozk_BS94",
    "TGoiZp6_JpIZ"
   ],
   "machine_shape": "hm",
   "name": "PPG2ABP_GitHub.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
